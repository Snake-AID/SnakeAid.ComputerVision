{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Snake-AID/SnakeAid.ComputerVision/blob/main/SnakeTraining_V5_YOLOv12_Khiem_Bbox5000.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E27294vCBzwF"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Snake-AID/SnakeAid.ComputerVision/blob/main/SnakeTraining_V4_YOLOv12_Khiem_Bbox5000.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaGgQiprFPvO"
      },
      "source": [
        "# Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kI_KzJP12XC"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1EREBTtjrgc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "HOME = os.getcwd()\n",
        "print(f\"üìÇ HOME: {HOME}\")\n",
        "\n",
        "PROJECT_NAME = \"SnakeTraining\"\n",
        "VERSION      = \"V4-1\"\n",
        "MODEL_ARCH   = \"YOLOv12\"\n",
        "AUTHOR       = \"Khiem\"\n",
        "DATA_INFO    = \"Bbox5291\"\n",
        "\n",
        "# L·∫•y th·ªùi gian hi·ªán t·∫°i theo m√∫i gi·ªù UTC+7\n",
        "utc_plus_7 = timezone(timedelta(hours=7))\n",
        "current_time = datetime.now(utc_plus_7).strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "# Gh√©p chu·ªói t√™n model k√®m timestamp\n",
        "notebook_name = f\"{PROJECT_NAME}_{VERSION}_{MODEL_ARCH}_{AUTHOR}_{DATA_INFO}_{current_time}\"\n",
        "\n",
        "print(f\"üìò Configured Model Name: {notebook_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "563dcec6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59ccbf61"
      },
      "source": [
        "### Define Dataset Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faba6092"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "from tqdm.notebook import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# --- CONFIG ---\n",
        "DRIVE_DATASET_PATH = '/content/drive/MyDrive/SnakeDataset/SnakeAid-YOLOv12-5291BBox'\n",
        "DATASET_PATH = '/content/snake_dataset'\n",
        "DATA_YAML_PATH = f'{DATASET_PATH}/data.yaml'\n",
        "\n",
        "def check_dataset_integrity(path):\n",
        "    \"\"\"Ki·ªÉm tra th√¥ng minh: Tr·∫£ v·ªÅ True n·∫øu data ƒë√£ ƒë·∫ßy ƒë·ªß\"\"\"\n",
        "    # 1. Check file c·∫•u h√¨nh\n",
        "    if not os.path.exists(f\"{path}/data.yaml\"):\n",
        "        return False\n",
        "\n",
        "    # 2. Check c√°c folder quan tr·ªçng (train, valid, test)\n",
        "    # N·∫øu b·∫•t k·ª≥ folder n√†o thi·∫øu ho·∫∑c r·ªóng -> coi nh∆∞ l·ªói -> copy l·∫°i\n",
        "    required_dirs = [\n",
        "        'train/images', 'train/labels',\n",
        "        'valid/images', 'valid/labels',\n",
        "        'test/images', 'test/labels'\n",
        "    ]\n",
        "\n",
        "    for d in required_dirs:\n",
        "        dir_path = os.path.join(path, d)\n",
        "        if not os.path.exists(dir_path):\n",
        "            return False\n",
        "        if not os.listdir(dir_path): # Folder r·ªóng\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def copy_file_worker(args):\n",
        "    \"\"\"H√†m worker ƒë·ªÉ copy 1 file (d√πng cho ƒëa lu·ªìng)\"\"\"\n",
        "    src, dst = args\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
        "        shutil.copy2(src, dst)\n",
        "    except Exception as e:\n",
        "        # C√≥ th·ªÉ log error n·∫øu c·∫ßn\n",
        "        pass\n",
        "\n",
        "def fast_copy_multithread(src_root, dst_root, workers=16):\n",
        "    \"\"\"Copy d·ªØ li·ªáu s·ª≠ d·ª•ng ƒëa lu·ªìng ƒë·ªÉ t·ªëi ∆∞u I/O\"\"\"\n",
        "    if os.path.exists(dst_root):\n",
        "        print(f\"üóëÔ∏è Ph√°t hi·ªán d·ªØ li·ªáu kh√¥ng to√†n v·∫πn. ƒêang x√≥a {dst_root}...\")\n",
        "        shutil.rmtree(dst_root)\n",
        "\n",
        "    print(\"üîç ƒêang qu√©t danh s√°ch file ngu·ªìn (Scanning)...\")\n",
        "    files_to_copy = []\n",
        "    for root, dirs, files in os.walk(src_root):\n",
        "        for file in files:\n",
        "            src_file = os.path.join(root, file)\n",
        "            rel_path = os.path.relpath(src_file, src_root)\n",
        "            dst_file = os.path.join(dst_root, rel_path)\n",
        "            files_to_copy.append((src_file, dst_file))\n",
        "\n",
        "    print(f\"üöÄ T√¨m th·∫•y {len(files_to_copy)} files. B·∫Øt ƒë·∫ßu copy t·ªëc ƒë·ªô cao (16 threads)...\")\n",
        "\n",
        "    # ThreadPoolExecutor gi√∫p request nhi·ªÅu file c√πng l√∫c t·ª´ Drive\n",
        "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
        "        list(tqdm(executor.map(copy_file_worker, files_to_copy), total=len(files_to_copy), unit=\"file\"))\n",
        "\n",
        "# --- MAIN FLOW ---\n",
        "if check_dataset_integrity(DATASET_PATH):\n",
        "    print(\"‚úÖ Smart Check: D·ªØ li·ªáu ƒë√£ ƒë·∫ßy ƒë·ªß v√† h·ª£p l·ªá t·∫°i Local. (Skipped Copy)\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Smart Check: D·ªØ li·ªáu thi·∫øu ho·∫∑c ch∆∞a c√≥. Ti·∫øn h√†nh copy...\")\n",
        "    try:\n",
        "        fast_copy_multithread(DRIVE_DATASET_PATH, DATASET_PATH)\n",
        "\n",
        "        # C·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n trong data.yaml\n",
        "        if os.path.exists(DATA_YAML_PATH):\n",
        "            with open(DATA_YAML_PATH, 'r') as f:\n",
        "                data_config = yaml.safe_load(f)\n",
        "\n",
        "            data_config['path'] = DATASET_PATH\n",
        "            # Fix ƒë∆∞·ªùng d·∫´n relative\n",
        "            for key in ['train', 'val', 'test']:\n",
        "                if key in data_config and isinstance(data_config[key], str):\n",
        "                    if 'drive' in data_config[key].lower():\n",
        "                        folder = 'valid' if key == 'val' else key\n",
        "                        data_config[key] = f'{folder}/images'\n",
        "\n",
        "            with open(DATA_YAML_PATH, 'w') as f:\n",
        "                yaml.dump(data_config, f)\n",
        "            print(\"‚úÖ C·∫•u h√¨nh data.yaml ho√†n t·∫•t!\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y data.yaml sau khi copy!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå L·ªói khi copy: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7808e264"
      },
      "outputs": [],
      "source": [
        "from rich import print\n",
        "from rich.panel import Panel\n",
        "from rich.table import Table\n",
        "from rich import box\n",
        "import os\n",
        "\n",
        "def check_dataset(root, folders=(\"train\", \"valid\", \"test\")):\n",
        "    table = Table(\n",
        "        title=\"Dataset Structure\",\n",
        "        box=box.SIMPLE_HEAD,\n",
        "        show_lines=False,\n",
        "        pad_edge=False\n",
        "    )\n",
        "\n",
        "    table.add_column(\"Component\", style=\"bold cyan\", justify=\"center\")\n",
        "    table.add_column(\"Status/Images\", justify=\"center\")\n",
        "    table.add_column(\"Labels\", justify=\"center\")\n",
        "\n",
        "    missing_files = False\n",
        "\n",
        "    yaml_path = os.path.join(root, \"data.yaml\")\n",
        "    yaml_exists = os.path.exists(yaml_path)\n",
        "\n",
        "    if not yaml_exists:\n",
        "        missing_files = True\n",
        "\n",
        "    table.add_row(\n",
        "        \"data.yaml\",\n",
        "        \"[green]FOUND\" if yaml_exists else \"[red]MISSING\",\n",
        "        \"-\"\n",
        "    )\n",
        "\n",
        "    for folder in folders:\n",
        "        fp = os.path.join(root, folder)\n",
        "        img = os.path.join(fp, \"images\")\n",
        "        lbl = os.path.join(fp, \"labels\")\n",
        "\n",
        "        img_exists = os.path.exists(img)\n",
        "        lbl_exists = os.path.exists(lbl)\n",
        "\n",
        "        if not img_exists or not lbl_exists:\n",
        "            missing_files = True\n",
        "\n",
        "        table.add_row(\n",
        "            folder.upper(),\n",
        "            \"[green]FOUND\" if img_exists else \"[red]MISSING\",\n",
        "            \"[green]FOUND\" if lbl_exists else \"[red]MISSING\",\n",
        "        )\n",
        "\n",
        "    panel = Panel.fit(\n",
        "        table,\n",
        "        title=\"üêç Snake Dataset Check\",\n",
        "        border_style=\"cyan\",\n",
        "        box=box.SIMPLE\n",
        "    )\n",
        "\n",
        "    print(panel)\n",
        "\n",
        "    if missing_files:\n",
        "        raise FileNotFoundError(\"Dataset structure is incomplete! Please check your dataset path and structure.\")\n",
        "\n",
        "check_dataset(DATASET_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54fd13ea"
      },
      "source": [
        "# Install YOLOv12 and SuperVision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4HSbchh19MH"
      },
      "source": [
        "## Install essential python package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8JKctYYXV3tv"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/sunsmarterjie/yolov12.git supervision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGUoLporEfTs"
      },
      "source": [
        "# Test the base models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P579Wz8j8dM"
      },
      "source": [
        "### Download example data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWKaiK4AWycO"
      },
      "outputs": [],
      "source": [
        "!wget https://media.roboflow.com/notebooks/examples/dog.jpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTntULfj-ly"
      },
      "source": [
        "### Run inference\n",
        "\n",
        "In the example, we're using the `yolov12l.pt` model, but you can experiment with different model sizes by simply swapping out the model name during initialization. Options include `yolov12n.pt`, `yolov12s.pt`, `yolov12m.pt`, `yolov12l.pt`, and `yolov12x.pt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFOfDnL_Ia8Y"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import supervision as sv\n",
        "\n",
        "\n",
        "image_path = f\"{HOME}/dog.jpeg\"\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "model = YOLO('yolov12l.pt')\n",
        "\n",
        "results = model(image, verbose=False)[0]\n",
        "detections = sv.Detections.from_ultralytics(results)\n",
        "\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "label_annotator = sv.LabelAnnotator()\n",
        "\n",
        "annotated_image = image.copy()\n",
        "annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections)\n",
        "annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)\n",
        "\n",
        "sv.plot_image(annotated_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONOK84CITP50"
      },
      "source": [
        "# Fine-tune YOLOv12 model\n",
        "\n",
        "We are now ready to fine-tune our YOLOv12 model. In the code below, we initialize the model using a starting checkpoint‚Äîhere, we use `yolov12s.yaml`, but you can replace it with any other model (e.g., `yolov12n.pt`, `yolov12m.pt`, `yolov12l.pt`, or `yolov12x.pt`) based on your preference. We set the training to run for 100 epochs in this example; however, you should adjust the number of epochs along with other hyperparameters such as batch size, image size, and augmentation settings (scale, mosaic, mixup, and copy-paste) based on your hardware capabilities and dataset size.\n",
        "\n",
        "**Note:** **Note that after training, you might encounter a `TypeError: argument of type 'PosixPath' is not iterable error` ‚Äî this is a known issue, but your model weights will still be saved, so you can safely proceed to running inference.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YBjWLSSQ_n1"
      },
      "outputs": [],
      "source": [
        "# from ultralytics import YOLO\n",
        "\n",
        "# model = YOLO('yolov12m.pt')\n",
        "# results = model.train(data=DATA_YAML_PATH, epochs=100, patience=10, cache=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU detect (auto select profile)"
      ],
      "metadata": {
        "id": "Hb-OaMV2Ksv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "A8lZy0qpUkh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "from rich import print\n",
        "from rich.panel import Panel\n",
        "from rich.table import Table\n",
        "from rich import box\n"
      ],
      "metadata": {
        "id": "tme1FCMuKueQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helpers + Rich logger**"
      ],
      "metadata": {
        "id": "gYwmI5BJUrNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_kaggle_env() -> bool:\n",
        "    return (\n",
        "        \"KAGGLE_URL_BASE\" in os.environ\n",
        "        or \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
        "        or os.path.exists(\"/kaggle\")\n",
        "    )\n",
        "\n",
        "def log_gpu_info():\n",
        "    table = Table(\n",
        "        title=\"GPU Runtime Information\",\n",
        "        box=box.SIMPLE_HEAVY,\n",
        "        show_header=True,\n",
        "        header_style=\"bold cyan\"\n",
        "    )\n",
        "    table.add_column(\"Metric\", style=\"bold\")\n",
        "    table.add_column(\"Value\")\n",
        "\n",
        "    if not torch.cuda.is_available():\n",
        "        table.add_row(\"CUDA Available\", \"‚ùå No\")\n",
        "        table.add_row(\"GPU Count\", \"0\")\n",
        "        print(table)\n",
        "        return\n",
        "\n",
        "    idx = torch.cuda.current_device()\n",
        "    props = torch.cuda.get_device_properties(idx)\n",
        "\n",
        "    total_gb = props.total_memory / (1024**3)\n",
        "    reserved_gb = torch.cuda.memory_reserved(idx) / (1024**3)\n",
        "    allocated_gb = torch.cuda.memory_allocated(idx) / (1024**3)\n",
        "    free_gb = total_gb - reserved_gb\n",
        "\n",
        "    table.add_row(\"CUDA Available\", \"‚úÖ Yes\")\n",
        "    table.add_row(\"GPU Name\", props.name)\n",
        "    table.add_row(\"GPU Count\", str(torch.cuda.device_count()))\n",
        "    table.add_row(\"VRAM Total\", f\"{total_gb:.2f} GB\")\n",
        "    table.add_row(\"VRAM Reserved\", f\"{reserved_gb:.2f} GB\")\n",
        "    table.add_row(\"VRAM Allocated\", f\"{allocated_gb:.2f} GB\")\n",
        "    table.add_row(\"VRAM Free (approx)\", f\"{free_gb:.2f} GB\")\n",
        "\n",
        "    print(table)"
      ],
      "metadata": {
        "id": "oEvGSZGkUq_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main detection (select profile)**"
      ],
      "metadata": {
        "id": "zAjKy0kZUvW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_training_profile():\n",
        "    kaggle = is_kaggle_env()\n",
        "\n",
        "    env_name = \"Kaggle\" if kaggle else \"Colab / Other\"\n",
        "    print(\n",
        "        Panel.fit(\n",
        "            f\"[bold]Environment:[/bold] {env_name}\",\n",
        "            title=\"Runtime Environment\",\n",
        "            border_style=\"green\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Log GPU info (log only)\n",
        "    log_gpu_info()\n",
        "\n",
        "    if not torch.cuda.is_available():\n",
        "        return \"CPU\"\n",
        "\n",
        "    gpu_name = torch.cuda.get_device_name(0).lower()\n",
        "\n",
        "    # Kaggle mapping\n",
        "    if kaggle:\n",
        "        if \"p100\" in gpu_name:\n",
        "            return \"KAGGLE_P100\"\n",
        "        if \"t4\" in gpu_name:\n",
        "            return \"KAGGLE_T4X2\"\n",
        "        return \"KAGGLE_P100\"\n",
        "\n",
        "    # Colab mapping\n",
        "    if \"a100\" in gpu_name:\n",
        "        return \"A100\"\n",
        "    if \"l4\" in gpu_name:\n",
        "        return \"L4\"\n",
        "    if \"t4\" in gpu_name:\n",
        "        return \"T4\"\n",
        "\n",
        "    return \"T4\"\n",
        "\n",
        "\n",
        "TRAIN_PROFILE = select_training_profile()\n",
        "\n",
        "print(\n",
        "    Panel.fit(\n",
        "        f\"[bold cyan]Selected training profile:[/bold cyan] [bold yellow]{TRAIN_PROFILE}[/bold yellow]\",\n",
        "        title=\"Profile Selection\",\n",
        "        border_style=\"cyan\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "Mo_Nf_zOUwEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training config dictionary (profiles)"
      ],
      "metadata": {
        "id": "eKjSQ7yEKywl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Param Config\n",
        "\n",
        "#### `epochs=200`\n",
        "\n",
        "* **Ch·ª©c nƒÉng:** X√°c ƒë·ªãnh s·ªë v√≤ng l·∫∑p hu·∫•n luy·ªán t·ªëi ƒëa m√† m√¥ h√¨nh s·∫Ω ch·∫°y tr√™n to√†n b·ªô t·∫≠p d·ªØ li·ªáu.\n",
        "* **L√Ω do l·ª±a ch·ªçn:** V·ªõi ƒë·ªëi t∆∞·ª£ng r·∫Øn th∆∞·ªùng nh·ªè v√† kh√≥ ph√¢n bi·ªát v·ªõi n·ªÅn, c√°c ch·ªâ s·ªë nh∆∞ mAP v√† Recall th∆∞·ªùng c·∫£i thi·ªán mu·ªôn, v√¨ v·∫≠y c·∫ßn nhi·ªÅu epoch h∆°n m·ª©c m·∫∑c ƒë·ªãnh (100) ƒë·ªÉ m√¥ h√¨nh h·ªôi t·ª• ƒë·∫ßy ƒë·ªß.\n",
        "\n",
        "---\n",
        "\n",
        "#### `patience=40`\n",
        "\n",
        "* **Ch·ª©c nƒÉng:** ƒêi·ªÅu khi·ªÉn c∆° ch·∫ø early stopping, m√¥ h√¨nh s·∫Ω d·ª´ng hu·∫•n luy·ªán n·∫øu metric validation kh√¥ng c·∫£i thi·ªán sau s·ªë epoch n√†y.\n",
        "* **L√Ω do l·ª±a ch·ªçn:** Trong b√†i to√°n SnakeAid, metric th∆∞·ªùng dao ƒë·ªông m·∫°nh do v·∫≠t th·ªÉ nh·ªè v√† n·ªÅn ph·ª©c t·∫°p, n√™n ƒë·∫∑t patience cao ƒë·ªÉ tr√°nh d·ª´ng hu·∫•n luy·ªán qu√° s·ªõm khi m√¥ h√¨nh ch∆∞a ƒë·∫°t tr·∫°ng th√°i t·ªëi ∆∞u.\n",
        "\n",
        "---\n",
        "\n",
        "#### `imgsz=896`\n",
        "\n",
        "* **Ch·ª©c nƒÉng:** Thi·∫øt l·∫≠p ƒë·ªô ph√¢n gi·∫£i ·∫£nh ƒë·∫ßu v√†o cho qu√° tr√¨nh hu·∫•n luy·ªán.\n",
        "* **L√Ω do l·ª±a ch·ªçn:** R·∫Øn trong ·∫£nh th∆∞·ªùng chi·∫øm r·∫•t √≠t pixel, tƒÉng ƒë·ªô ph√¢n gi·∫£i gi√∫p m√¥ h√¨nh gi·ªØ l·∫°i nhi·ªÅu chi ti·∫øt kh√¥ng gian h∆°n, t·ª´ ƒë√≥ c·∫£i thi·ªán kh·∫£ nƒÉng ph√°t hi·ªán r·∫Øn nh·ªè ho·∫∑c ·ªü xa.\n",
        "\n",
        "---\n",
        "\n",
        "#### `batch=16`\n",
        "\n",
        "* **Ch·ª©c nƒÉng:** X√°c ƒë·ªãnh s·ªë l∆∞·ª£ng ·∫£nh ƒë∆∞·ª£c x·ª≠ l√Ω trong m·ªôt b∆∞·ªõc c·∫≠p nh·∫≠t gradient.\n",
        "* **L√Ω do l·ª±a ch·ªçn:** Batch size v·ª´a ph·∫£i gi√∫p c√¢n b·∫±ng gi·ªØa ƒë·ªô ·ªïn ƒë·ªãnh c·ªßa gradient v√† kh·∫£ nƒÉng h·ªçc c√°c v·∫≠t th·ªÉ nh·ªè, tr√°nh hi·ªán t∆∞·ª£ng l√†m m∆∞·ª£t qu√° m·ª©c khi batch qu√° l·ªõn.\n",
        "\n",
        "---\n",
        "\n",
        "#### `cache=True`\n",
        "\n",
        "* **Ch·ª©c nƒÉng:** L∆∞u d·ªØ li·ªáu hu·∫•n luy·ªán v√†o b·ªô nh·ªõ ƒë·ªám ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô ƒë·ªçc d·ªØ li·ªáu.\n",
        "* **L√Ω do l·ª±a ch·ªçn:** Dataset ƒë∆∞·ª£c l∆∞u tr√™n Google Drive khi train b·∫±ng Colab, vi·ªác b·∫≠t cache gi√∫p gi·∫£m ƒë·ªô tr·ªÖ I/O v√† r√∫t ng·∫Øn th·ªùi gian hu·∫•n luy·ªán t·ªïng th·ªÉ.\n",
        "\n",
        "---\n",
        "\n",
        "#### `workers=8`\n",
        "\n",
        "* **Ch·ª©c nƒÉng:** X√°c ƒë·ªãnh s·ªë ti·∫øn tr√¨nh CPU song song ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒë·ªçc d·ªØ li·ªáu, ti·ªÅn x·ª≠ l√Ω ·∫£nh v√† √°p d·ª•ng c√°c augmentation tr∆∞·ªõc khi d·ªØ li·ªáu ƒë∆∞·ª£c ƒë∆∞a v√†o GPU trong qu√° tr√¨nh hu·∫•n luy·ªán.\n",
        "* **L√Ω do l·ª±a ch·ªçn:** V·ªõi ·∫£nh ƒë·ªô ph√¢n gi·∫£i cao (`imgsz=896`) v√† d·ªØ li·ªáu l∆∞u tr√™n Google Drive khi hu·∫•n luy·ªán b·∫±ng Colab A100, vi·ªác s·ª≠ d·ª•ng `workers=8` gi√∫p t·∫≠n d·ª•ng hi·ªáu qu·∫£ t√†i nguy√™n CPU, tr√°nh t√¨nh tr·∫°ng GPU ph·∫£i ch·ªù d·ªØ li·ªáu v√† ƒë·∫£m b·∫£o t·ªëc ƒë·ªô hu·∫•n luy·ªán ·ªïn ƒë·ªãnh cho b√†i to√°n nh·∫≠n di·ªán r·∫Øn nh·ªè v√† ·ªü xa.\n",
        "\n",
        "---\n",
        "\n",
        "#### `cos_lr=True`\n",
        "\n",
        "* **Ch·ª©c nƒÉng:** S·ª≠ d·ª•ng chi·∫øn l∆∞·ª£c gi·∫£m learning rate theo h√†m cosine trong qu√° tr√¨nh hu·∫•n luy·ªán.\n",
        "* **L√Ω do l·ª±a ch·ªçn:** Cosine learning rate gi√∫p fine-tune m∆∞·ª£t h∆°n ·ªü c√°c epoch sau, gi·∫£m nguy c∆° dao ƒë·ªông m·∫°nh v√† gi√∫p m√¥ h√¨nh ·ªïn ƒë·ªãnh khi t·ªëi ∆∞u cho c√°c v·∫≠t th·ªÉ nh·ªè."
      ],
      "metadata": {
        "id": "74RuP2kuVXFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIGS = {\n",
        "\n",
        "    # Quick sanity / pipeline check\n",
        "    \"DEBUG\": dict(\n",
        "        epochs=1,\n",
        "        patience=0,\n",
        "        imgsz=512,\n",
        "        batch=2,\n",
        "        accumulate=1,\n",
        "        cache=False,\n",
        "        workers=1,\n",
        "    ),\n",
        "\n",
        "    # CPU-only\n",
        "    \"CPU\": dict(\n",
        "        epochs=120,\n",
        "        patience=30,\n",
        "        imgsz=640,\n",
        "        batch=2,\n",
        "        accumulate=4,     # effective batch = 8\n",
        "        cache=\"disk\",\n",
        "        workers=2,\n",
        "    ),\n",
        "\n",
        "    # Colab Free (Tesla T4 16GB VRAM)\n",
        "    \"T4\": dict(\n",
        "        epochs=80,\n",
        "        patience=20,\n",
        "        imgsz=768,\n",
        "        batch=4,\n",
        "        accumulate=2,     # effective batch = 8\n",
        "        cache=\"disk\",\n",
        "        workers=2,\n",
        "    ),\n",
        "\n",
        "    # Kaggle GPU P100 (16GB VRAM)\n",
        "    \"KAGGLE_P100\": dict(\n",
        "        epochs=80,\n",
        "        patience=20,\n",
        "        imgsz=768,\n",
        "        batch=4,\n",
        "        accumulate=2,     # effective batch = 8\n",
        "        cache=\"disk\",\n",
        "        workers=2,\n",
        "    ),\n",
        "\n",
        "    # Kaggle GPU T4 x2 (treated as single T4 unless DDP enabled)\n",
        "    \"KAGGLE_T4X2\": dict(\n",
        "        epochs=80,\n",
        "        patience=20,\n",
        "        imgsz=768,\n",
        "        batch=4,\n",
        "        accumulate=2,     # effective batch = 8\n",
        "        cache=\"disk\",\n",
        "        workers=2,\n",
        "    ),\n",
        "\n",
        "    # Colab Pro (NVIDIA L4 24GB VRAM)\n",
        "    \"L4\": dict(\n",
        "        epochs=120,\n",
        "        patience=25,\n",
        "        imgsz=768,\n",
        "        batch=8,\n",
        "        accumulate=1,\n",
        "        cache=True,\n",
        "        workers=4,\n",
        "    ),\n",
        "\n",
        "    # Colab Pro (A100 40GB VRAM)\n",
        "    \"A100\": dict(\n",
        "        epochs=200,\n",
        "        patience=40,\n",
        "        imgsz=896,\n",
        "        batch=16,\n",
        "        accumulate=1,\n",
        "        cache=True,\n",
        "        workers=8,\n",
        "    ),\n",
        "}\n",
        "\n",
        "cfg = CONFIGS[TRAIN_PROFILE]\n",
        "\n",
        "print(\"üß† Loaded training configuration:\")\n",
        "for k, v in cfg.items():\n",
        "    print(f\"  {k}: {v}\")"
      ],
      "metadata": {
        "id": "0VC7jht8KxOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM9WZ7AoSmvs"
      },
      "source": [
        "### Param Config\n",
        "\n",
        "#### `mixup=0.0`\n",
        "\n",
        "* **Ch·ª©c nƒÉng:** ƒêi·ªÅu ch·ªânh m·ª©c ƒë·ªô s·ª≠ d·ª•ng augmentation mixup (tr·ªôn hai ·∫£nh v√† nh√£n).\n",
        "* **L√Ω do l·ª±a ch·ªçn:** Mixup c√≥ th·ªÉ t·∫°o ra c√°c h√¨nh ·∫£nh kh√¥ng th·ª±c t·∫ø ƒë·ªëi v·ªõi ƒë·ªëi t∆∞·ª£ng d√†i v√† m·∫£nh nh∆∞ r·∫Øn, l√†m gi·∫£m ch·∫•t l∆∞·ª£ng h·ªçc, v√¨ v·∫≠y ƒë∆∞·ª£c t·∫Øt ho√†n to√†n trong b√†i to√°n n√†y.\n",
        "\n",
        "---\n",
        "\n",
        "#### `copy_paste=0.0`\n",
        "\n",
        "* **Ch·ª©c nƒÉng:** ƒêi·ªÅu khi·ªÉn augmentation copy-paste, trong ƒë√≥ ƒë·ªëi t∆∞·ª£ng ƒë∆∞·ª£c c·∫Øt v√† d√°n sang ·∫£nh kh√°c.\n",
        "* **L√Ω do l·ª±a ch·ªçn:** Copy-paste c√≥ th·ªÉ t·∫°o ra c√°c v·ªã tr√≠ r·∫Øn kh√¥ng t·ª± nhi√™n trong ·∫£nh, g√¢y nhi·ªÖu cho m√¥ h√¨nh, n√™n ƒë∆∞·ª£c t·∫Øt ƒë·ªÉ gi·ªØ t√≠nh th·ª±c t·∫ø c·ªßa d·ªØ li·ªáu hu·∫•n luy·ªán."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2Zk0Wk5Smg4"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load pretrained model\n",
        "model = YOLO(\"yolov12s.pt\")\n",
        "\n",
        "results = model.train(\n",
        "    data=(DATA_YAML_PATH),\n",
        "    project=f\"{HOME}/runs/detect\",\n",
        "    name='train',\n",
        "    exist_ok=True,\n",
        "\n",
        "    # epochs=cfg[\"epochs\"],\n",
        "    # patience=cfg[\"patience\"],\n",
        "    epochs=17,\n",
        "    patience=0,\n",
        "    imgsz=cfg[\"imgsz\"],\n",
        "    batch=cfg[\"batch\"],\n",
        "    accumulate=cfg[\"accumulate\"],\n",
        "    cache=cfg[\"cache\"],\n",
        "    workers=cfg[\"workers\"],\n",
        "    cos_lr=True,\n",
        "\n",
        "    # Snake-specific augmentation control\n",
        "    mixup=0.0,\n",
        "    copy_paste=0.0,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training finished\")\n",
        "print(f\"üìÅ Weights saved to: runs/detect/{TRAIN_PROFILE.lower()}/weights/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_WJBGEeXRcE"
      },
      "source": [
        "### Save model file into google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df00f760"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define the source path of the best trained model\n",
        "source_path = f'{HOME}/runs/detect/train/weights/best.pt'\n",
        "\n",
        "# Define the destination directory in Google Drive\n",
        "destination_dir = '/content/drive/MyDrive/SnakeAIModels'\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "# S·ª≠ d·ª•ng bi·∫øn 'notebook_name' ƒë√£ ƒë∆∞·ª£c define ·ªü cell Setup ƒë·∫ßu notebook\n",
        "print(f\"üíæ Saving model for notebook: {notebook_name}\")\n",
        "destination_filename = f\"{notebook_name}.pt\"\n",
        "destination_path = os.path.join(destination_dir, destination_filename)\n",
        "\n",
        "# Copy the model file to Google Drive\n",
        "if os.path.exists(source_path):\n",
        "    shutil.copy(source_path, destination_path)\n",
        "    print(f\"‚úÖ Trained model saved to: {destination_path}\")\n",
        "else:\n",
        "    print(f\"‚ùå Source model not found at {source_path}. Make sure training has finished successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMhVugreT5A5"
      },
      "source": [
        "# Evaluate fine-tuned YOLOv12 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "387eb67d"
      },
      "source": [
        "### Inspect Training Output Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm1FRMzDTYoR"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!ls {HOME}/runs/detect/train/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8f570b2"
      },
      "source": [
        "### Display Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0W8FDBVZbRdo"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image(filename=f'{HOME}/runs/detect/train/confusion_matrix.png', width=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97fb1f6f"
      },
      "source": [
        "### Display Training Results Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9y8zJ8nlBUT"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image(filename=f'{HOME}/runs/detect/train/results.png', width=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb640378"
      },
      "source": [
        "### Load Test Dataset and Display Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2KT2JlGVS_-"
      },
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "\n",
        "ds = sv.DetectionDataset.from_yolo(\n",
        "    images_directory_path=f\"{DATASET_PATH}/test/images\",\n",
        "    annotations_directory_path=f\"{DATASET_PATH}/test/labels\",\n",
        "    data_yaml_path=DATA_YAML_PATH\n",
        ")\n",
        "\n",
        "ds.classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c65a7126"
      },
      "source": [
        "### Calculate Mean Average Precision (mAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBZCaDvZWpHc"
      },
      "outputs": [],
      "source": [
        "from supervision.metrics import MeanAveragePrecision\n",
        "\n",
        "model = YOLO(f'{HOME}/runs/detect/train/weights/best.pt')\n",
        "\n",
        "predictions = []\n",
        "targets = []\n",
        "\n",
        "for _, image, target in ds:\n",
        "    results = model(image, verbose=False)[0]\n",
        "    detections = sv.Detections.from_ultralytics(results)\n",
        "\n",
        "    predictions.append(detections)\n",
        "    targets.append(target)\n",
        "\n",
        "map = MeanAveragePrecision().update(predictions, targets).compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29f5af9f"
      },
      "source": [
        "### Print mAP Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0U9bcrjBXPT2"
      },
      "outputs": [],
      "source": [
        "print(\"mAP 50:95\", map.map50_95)\n",
        "print(\"mAP 50\", map.map50)\n",
        "print(\"mAP 75\", map.map75)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aa40512"
      },
      "source": [
        "### Plot mAP Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmD1SFofXf_o"
      },
      "outputs": [],
      "source": [
        "map.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAObQt4nlKLD"
      },
      "source": [
        "# Run inference with fine-tuned YOLOv12 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRMVH1pnoXgD"
      },
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "\n",
        "model = YOLO(f'{HOME}/runs/detect/train/weights/best.pt')\n",
        "\n",
        "ds = sv.DetectionDataset.from_yolo(\n",
        "    images_directory_path=f\"{DATASET_PATH}/test/images\",\n",
        "    annotations_directory_path=f\"{DATASET_PATH}/test/labels\",\n",
        "    data_yaml_path=DATA_YAML_PATH\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S97p_O7YPsa"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "i = random.randint(0, len(ds))\n",
        "\n",
        "image_path, image, target = ds[i]\n",
        "\n",
        "results = model(image, verbose=False)[0]\n",
        "detections = sv.Detections.from_ultralytics(results).with_nms()\n",
        "\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "label_annotator = sv.LabelAnnotator()\n",
        "\n",
        "annotated_image = image.copy()\n",
        "annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections)\n",
        "annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)\n",
        "\n",
        "sv.plot_image(annotated_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Yb269euE1km"
      },
      "source": [
        "# Call t·∫Øt laptop sau khi train xong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H18osLxaE41S"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "WEBHOOK_URL = \"https://forcepslike-lawanda-semicalcined.ngrok-free.dev/done\"  # ƒë·ªïi URL c·ªßa b·∫°n\n",
        "\n",
        "try:\n",
        "    r = requests.post(WEBHOOK_URL)\n",
        "    print(\"ƒê√£ g·ª≠i t√≠n hi·ªáu shutdown v·ªÅ PC!\")\n",
        "except Exception as e:\n",
        "    print(\"Kh√¥ng g·ª≠i ƒë∆∞·ª£c webhook:\", e)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}